{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "We will take a first-pass at evaluating or technique to start understanding its efficacy. We will existing CNN architectures and evaluate its performance on our interested categories with and without using our interested categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "print('TensorFlow Version: ', tf.__version__)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for training & validation\n",
    "INPUT_SHAPE = (64, 64, 3)\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.10\n",
    "TRAIN_STEPS_PER_EPOCH = 5000\n",
    "TEST_STEPS = 500\n",
    "NUM_EPOCHS = 100\n",
    "# Currently reduced\n",
    "NUM_LABELS=22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utilities and helper functions\n",
    "# NOTE: Copied from clustering NB\n",
    "def load_metadata(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return [x.strip().split('\\t') for x in f.readlines()]\n",
    "    \n",
    "@tf.function\n",
    "def decode_img(image):\n",
    "    img = tf.image.decode_jpeg(image, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return tf.image.resize(img, [64, 64])\n",
    "\n",
    "@tf.function\n",
    "def load_image_data(path, label):\n",
    "    img_data = tf.io.read_file(path)\n",
    "    img = decode_img(img_data)\n",
    "    return img, label\n",
    "    \n",
    "def load_labels(metadata):\n",
    "    labels = np.array([x[1] for x in metadata])\n",
    "    distinct_labels = np.array([[x] for x in set(labels)])\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoder.fit(distinct_labels)\n",
    "    y_train = encoder.transform([[x] for x in labels])\n",
    "    return (y_train, encoder)\n",
    "\n",
    "# Load labels with an existing encoder\n",
    "def load_labels_with_encoder(metadata, encoder):\n",
    "    return encoder.transform([[x[1]] for x in metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions for three models: (i) custom, simple CNN, (ii) MobileNet + FCs, and (iii) VGG16 + FCs\n",
    "def get_simplecnn(input_shape=INPUT_SHAPE):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(512, (3, 3), (1, 1), input_shape=input_shape, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(512, (2, 2), (1, 1), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(256, (2, 2), (1, 1), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(256, (2, 2), (1, 1), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(NUM_LABELS, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "def get_mobilenet(input_shape=INPUT_SHAPE):\n",
    "    application = tf.keras.applications.MobileNet(input_shape=input_shape, include_top=False)\n",
    "    for i in range(len(application.layers)):\n",
    "        application.layers[i].trainable = False\n",
    "        \n",
    "    return tf.keras.Sequential([\n",
    "        application,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(NUM_LABELS, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def get_vgg16(input_shape=INPUT_SHAPE):\n",
    "    application = tf.keras.applications.VGG16(input_shape=input_shape, include_top=False)\n",
    "    for i in range(len(application.layers)):\n",
    "        application.layers[i].trainable = False\n",
    "        \n",
    "    return tf.keras.Sequential([\n",
    "        application,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1024, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(NUM_LABELS, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplecnn = get_simplecnn()\n",
    "# simplecnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenet_1.00_224 (Model)   (None, 2, 2, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 22)                2838      \n",
      "=================================================================\n",
      "Total params: 3,379,414\n",
      "Trainable params: 150,550\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobilenet = get_mobilenet()\n",
    "mobilenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 = get_vgg16()\n",
    "# vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into memory...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Remove hardcoding\n",
    "print('Loading data into memory...')\n",
    "train_metadata = load_metadata('./metadata_output/filtered_train_metadata.txt')\n",
    "(y_train, encoder) = load_labels(train_metadata)\n",
    "\n",
    "# Interested indices for test data filtering\n",
    "interested_categories = ['n01882714', 'n04562935']\n",
    "interested_one_hot = encoder.transform([[x] for x in interested_categories])\n",
    "interested_indices = np.array([x[1] for x in np.argwhere(interested_one_hot == 1)])\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding sanity checks;\n",
    "# assert(len(train_metadata) == len(y_train))\n",
    "# assert(len(set(y_train)) == 200)\n",
    "assert(np.count_nonzero(y_train == 1) == len(train_metadata))\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_paths_and_labels(train_metadata, y_train):\n",
    "    return [(train_metadata[x][0], y_train[x]) for x in range(len(y_train))]\n",
    "\n",
    "# TODO: This is sort of weird/dangerous.. we shouldn't be using a global for this\n",
    "# since it changes per training run\n",
    "num_validation = 0\n",
    "def shuffle_and_split_data(train_metadata, y_train):\n",
    "    global num_validation\n",
    "    # Get all data\n",
    "    paths_and_labels = join_paths_and_labels(train_metadata, y_train)\n",
    "    print('Num. Total Images: ', len(paths_and_labels))\n",
    "\n",
    "    # Split data into train and validation sets\n",
    "    np.random.shuffle(paths_and_labels)\n",
    "    num_validation = int(len(paths_and_labels) * VALIDATION_SPLIT)\n",
    "    train_paths_and_labels = paths_and_labels[num_validation:]\n",
    "    validation_paths_and_labels = paths_and_labels[:num_validation]\n",
    "    print('Num. Train Images: ', len(train_paths_and_labels))\n",
    "    print('Num. Validation Images: ', len(validation_paths_and_labels))\n",
    "    \n",
    "    return (train_paths_and_labels, validation_paths_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) BASELINE MODEL: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Total Images:  7409\n",
      "Num. Train Images:  6669\n",
      "Num. Validation Images:  740\n"
     ]
    }
   ],
   "source": [
    "(train_paths_and_labels, validation_paths_and_labels) = shuffle_and_split_data(train_metadata, y_train)\n",
    "\n",
    "# Convert training set into a TF dataset via generator\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train[0])]))\n",
    ")\n",
    "train_dataset = train_dataset.map(lambda x,y: load_image_data(x, y), \n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Convert validation set into a TF dataset via generator\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: validation_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train[0])]))\n",
    ")\n",
    "validation_dataset = validation_dataset.map(lambda x,y: load_image_data(x, y), \n",
    "                                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "validation_dataset = validation_dataset.cache()\n",
    "validation_dataset = validation_dataset.repeat()\n",
    "validation_dataset = validation_dataset.batch(1)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Set test and train to dataset size...\n",
    "TRAIN_STEPS_PER_EPOCH = len(train_paths_and_labels)\n",
    "TEST_STEPS = len(validation_paths_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, validation_dataset, name):    \n",
    "    # Compile model                                                                                                      \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=4e-4),                                                           \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),                                  \n",
    "                  metrics=['accuracy'])      \n",
    "    \n",
    "    # Stop early if we're not making good progress                                                                           \n",
    "    early_stop_monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss',                                                                                              \n",
    "                                                          restore_best_weights=True,                                                                                       \n",
    "                                                          patience=10)   \n",
    "\n",
    "    # Prepare for checkpoints            \n",
    "    checkpoint_path = './checkpoints/' + name + '/cp-{epoch:04d}.ckpt'                                   \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                                                    \n",
    "                                                     verbose=1,                                                                                                   \n",
    "                                                     save_weights_only=True,                                                                                     \n",
    "                                                     save_freq=2500000)\n",
    "\n",
    "    # Tensorboard                                                                                                        \n",
    "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")                                              \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    history = model.fit(x=train_dataset,\n",
    "                        epochs=NUM_EPOCHS,                                                                                                  \n",
    "                        steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "                        callbacks=[tensorboard_callback, cp_callback, early_stop_monitor],\n",
    "                        use_multiprocessing=True,\n",
    "                        validation_steps=num_validation,\n",
    "                        validation_data=validation_dataset,\n",
    "                        shuffle=True)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 6669 steps, validate for 740 steps\n",
      "Epoch 1/100\n",
      "   1/6669 [..............................] - ETA: 15:45:00 - loss: 3.0926 - accuracy: 0.0781WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.109019). Check your callbacks.\n",
      "6669/6669 [==============================] - 149s 22ms/step - loss: 2.5013 - accuracy: 0.6712 - val_loss: 2.5864 - val_accuracy: 0.5838\n",
      "Epoch 2/100\n",
      "6669/6669 [==============================] - 141s 21ms/step - loss: 2.3346 - accuracy: 0.8356 - val_loss: 2.6126 - val_accuracy: 0.5527\n",
      "Epoch 3/100\n",
      "6669/6669 [==============================] - 143s 21ms/step - loss: 2.2949 - accuracy: 0.8738 - val_loss: 2.5794 - val_accuracy: 0.5851\n",
      "Epoch 4/100\n",
      "6669/6669 [==============================] - 140s 21ms/step - loss: 2.2574 - accuracy: 0.9108 - val_loss: 2.5887 - val_accuracy: 0.5757\n",
      "Epoch 5/100\n",
      "6669/6669 [==============================] - 142s 21ms/step - loss: 2.2476 - accuracy: 0.9198 - val_loss: 2.5927 - val_accuracy: 0.5716\n",
      "Epoch 6/100\n",
      "5715/6669 [========================>.....] - ETA: 19s - loss: 2.2424 - accuracy: 0.9248\n",
      "Epoch 00006: saving model to ./checkpoints/mobilenet_imbalanced/cp-0006.ckpt\n",
      "6669/6669 [==============================] - 143s 21ms/step - loss: 2.2421 - accuracy: 0.9251 - val_loss: 2.5739 - val_accuracy: 0.5905\n",
      "Epoch 7/100\n",
      "6669/6669 [==============================] - 141s 21ms/step - loss: 2.2388 - accuracy: 0.9282 - val_loss: 2.5908 - val_accuracy: 0.5797\n",
      "Epoch 8/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.2360 - accuracy: 0.9308 - val_loss: 2.5875 - val_accuracy: 0.5797\n",
      "Epoch 9/100\n",
      "6669/6669 [==============================] - 141s 21ms/step - loss: 2.2344 - accuracy: 0.9323 - val_loss: 2.5857 - val_accuracy: 0.5811\n",
      "Epoch 10/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.2327 - accuracy: 0.9340 - val_loss: 2.5695 - val_accuracy: 0.5973\n",
      "Epoch 11/100\n",
      "6669/6669 [==============================] - 142s 21ms/step - loss: 2.2315 - accuracy: 0.9352 - val_loss: 2.5853 - val_accuracy: 0.5824\n",
      "Epoch 12/100\n",
      "4764/6669 [====================>.........] - ETA: 38s - loss: 2.2300 - accuracy: 0.9366\n",
      "Epoch 00012: saving model to ./checkpoints/mobilenet_imbalanced/cp-0012.ckpt\n",
      "6669/6669 [==============================] - 142s 21ms/step - loss: 2.2300 - accuracy: 0.9365 - val_loss: 2.5890 - val_accuracy: 0.5784\n",
      "Epoch 13/100\n",
      "6669/6669 [==============================] - 141s 21ms/step - loss: 2.2293 - accuracy: 0.9371 - val_loss: 2.5867 - val_accuracy: 0.5824\n",
      "Epoch 14/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.2284 - accuracy: 0.9380 - val_loss: 2.5834 - val_accuracy: 0.5865\n",
      "Epoch 15/100\n",
      "6669/6669 [==============================] - 140s 21ms/step - loss: 2.2279 - accuracy: 0.9385 - val_loss: 2.5794 - val_accuracy: 0.5878\n",
      "Epoch 16/100\n",
      "6669/6669 [==============================] - 140s 21ms/step - loss: 2.2270 - accuracy: 0.9393 - val_loss: 2.5819 - val_accuracy: 0.5824\n",
      "Epoch 17/100\n",
      "6669/6669 [==============================] - 140s 21ms/step - loss: 2.2267 - accuracy: 0.9396 - val_loss: 2.5767 - val_accuracy: 0.5932\n",
      "Epoch 18/100\n",
      "3814/6669 [================>.............] - ETA: 56s - loss: 2.2265 - accuracy: 0.9398\n",
      "Epoch 00018: saving model to ./checkpoints/mobilenet_imbalanced/cp-0018.ckpt\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.2263 - accuracy: 0.9400 - val_loss: 2.5827 - val_accuracy: 0.5811\n",
      "Epoch 19/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.2257 - accuracy: 0.9407 - val_loss: 2.5782 - val_accuracy: 0.5905\n",
      "Epoch 20/100\n",
      "6669/6669 [==============================] - 138s 21ms/step - loss: 2.2252 - accuracy: 0.9411 - val_loss: 2.5928 - val_accuracy: 0.5676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f67605a9e50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and save model\n",
    "train_model(mobilenet, train_dataset, validation_dataset, 'mobilenet_imbalanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dennis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/mobilenet_imbalanced/assets\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join('models', 'mobilenet_imbalanced')):\n",
    "    os.makedirs(os.path.join('models', 'mobilenet_imbalanced'))\n",
    "    \n",
    "mobilenet.save(os.path.join('models', 'mobilenet_imbalanced'))\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) MOBILENET + STANDARD AUGMENTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "mobilenet_std_aug = get_mobilenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: We have to somehow incorporate the below with tf.Datasets\n",
    "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#     rotation_range=45,\n",
    "#     width_shift_range=0.4,\n",
    "#     height_shift_range=0.4,\n",
    "#     zoom_range=[0.4, 1.6],\n",
    "#     horizontal_flip=True,\n",
    "#     brightness_range=(0.6, 1.4),\n",
    "#     fill_mode='nearest',\n",
    "# )\n",
    "\n",
    "# NOTE: Apply a map function to perform transformations rather than using ImageDataGen\n",
    "@tf.function\n",
    "def std_augment_image(img_tensor, label):\n",
    "    transformed = img_tensor\n",
    "    # Random rotation\n",
    "#     if tf.random.uniform([]) <= 0.8:\n",
    "#         angle = tf.random.uniform([]) * 45.0\n",
    "#         transformed = tfa.image.rotate(transformed, angle)\n",
    "    # Random zoom (5%)\n",
    "    if tf.random.uniform([]) <= 0.05:\n",
    "        crop_size = tf.random.uniform([], minval=0.4, maxval=0.8) * 64.0\n",
    "        transformed = tf.image.resize(tf.image.random_crop(transformed, [crop_size, crop_size, 3]), [64, 64])\n",
    "    # Random brightness adjustment\n",
    "    if tf.random.uniform([]) <= 0.5:\n",
    "        transformed = tf.image.random_brightness(transformed, 0.6)\n",
    "    # Random horizontal flip\n",
    "    transformed = tf.image.random_flip_up_down(transformed)\n",
    "    return (transformed, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redfine train dataset\n",
    "# Convert training set into a TF dataset via generator\n",
    "train_dataset_std_aug = tf.data.Dataset.from_generator(\n",
    "    lambda: train_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train[0])]))\n",
    ")\n",
    "train_dataset_std_aug = train_dataset_std_aug.map(lambda x,y: load_image_data(x, y), \n",
    "                                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset_std_aug = train_dataset_std_aug.cache()\n",
    "train_dataset_std_aug = train_dataset_std_aug.map(std_augment_image,\n",
    "                                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset_std_aug = train_dataset_std_aug.repeat()\n",
    "train_dataset_std_aug = train_dataset_std_aug.batch(BATCH_SIZE)\n",
    "train_dataset_std_aug = train_dataset_std_aug.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Set test and train to dataset size...\n",
    "TRAIN_STEPS_PER_EPOCH = len(train_paths_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 6669 steps, validate for 740 steps\n",
      "Epoch 1/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.6425 - accuracy: 0.5266 - val_loss: 2.6958 - val_accuracy: 0.4757\n",
      "Epoch 2/100\n",
      "6669/6669 [==============================] - 137s 21ms/step - loss: 2.5538 - accuracy: 0.6136 - val_loss: 2.6713 - val_accuracy: 0.4973\n",
      "Epoch 3/100\n",
      "6669/6669 [==============================] - 137s 21ms/step - loss: 2.5099 - accuracy: 0.6572 - val_loss: 2.6582 - val_accuracy: 0.5054\n",
      "Epoch 4/100\n",
      "6669/6669 [==============================] - 137s 20ms/step - loss: 2.4795 - accuracy: 0.6877 - val_loss: 2.6191 - val_accuracy: 0.5514\n",
      "Epoch 5/100\n",
      "6669/6669 [==============================] - 138s 21ms/step - loss: 2.4417 - accuracy: 0.7258 - val_loss: 2.6111 - val_accuracy: 0.5595\n",
      "Epoch 6/100\n",
      "5717/6669 [========================>.....] - ETA: 18s - loss: 2.4289 - accuracy: 0.7379\n",
      "Epoch 00006: saving model to ./checkpoints/mobilenet_imbalanced_std_aug/cp-0006.ckpt\n",
      "6669/6669 [==============================] - 137s 21ms/step - loss: 2.4275 - accuracy: 0.7392 - val_loss: 2.6040 - val_accuracy: 0.5649\n",
      "Epoch 7/100\n",
      "6669/6669 [==============================] - 138s 21ms/step - loss: 2.4132 - accuracy: 0.7534 - val_loss: 2.5924 - val_accuracy: 0.5730\n",
      "Epoch 8/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.4062 - accuracy: 0.7604 - val_loss: 2.5874 - val_accuracy: 0.5784\n",
      "Epoch 9/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3998 - accuracy: 0.7668 - val_loss: 2.5976 - val_accuracy: 0.5689\n",
      "Epoch 10/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3965 - accuracy: 0.7699 - val_loss: 2.5988 - val_accuracy: 0.5649\n",
      "Epoch 11/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3938 - accuracy: 0.7724 - val_loss: 2.5960 - val_accuracy: 0.5689\n",
      "Epoch 12/100\n",
      "4766/6669 [====================>.........] - ETA: 37s - loss: 2.3907 - accuracy: 0.7756\n",
      "Epoch 00012: saving model to ./checkpoints/mobilenet_imbalanced_std_aug/cp-0012.ckpt\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3903 - accuracy: 0.7759 - val_loss: 2.5914 - val_accuracy: 0.5770\n",
      "Epoch 13/100\n",
      "6669/6669 [==============================] - 140s 21ms/step - loss: 2.3888 - accuracy: 0.7775 - val_loss: 2.5966 - val_accuracy: 0.5689\n",
      "Epoch 14/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3859 - accuracy: 0.7802 - val_loss: 2.5918 - val_accuracy: 0.5730\n",
      "Epoch 15/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3846 - accuracy: 0.7817 - val_loss: 2.5918 - val_accuracy: 0.5730\n",
      "Epoch 16/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3845 - accuracy: 0.7817 - val_loss: 2.5914 - val_accuracy: 0.5757\n",
      "Epoch 17/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3822 - accuracy: 0.7838 - val_loss: 2.5950 - val_accuracy: 0.5743\n",
      "Epoch 18/100\n",
      "3814/6669 [================>.............] - ETA: 55s - loss: 2.3808 - accuracy: 0.7853\n",
      "Epoch 00018: saving model to ./checkpoints/mobilenet_imbalanced_std_aug/cp-0018.ckpt\n",
      "6669/6669 [==============================] - 138s 21ms/step - loss: 2.3813 - accuracy: 0.7848 - val_loss: 2.5760 - val_accuracy: 0.5905\n",
      "Epoch 19/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3803 - accuracy: 0.7857 - val_loss: 2.5885 - val_accuracy: 0.5784\n",
      "Epoch 20/100\n",
      "6669/6669 [==============================] - 138s 21ms/step - loss: 2.3797 - accuracy: 0.7863 - val_loss: 2.5925 - val_accuracy: 0.5730\n",
      "Epoch 21/100\n",
      "6669/6669 [==============================] - 138s 21ms/step - loss: 2.3774 - accuracy: 0.7886 - val_loss: 2.5958 - val_accuracy: 0.5676\n",
      "Epoch 22/100\n",
      "6669/6669 [==============================] - 138s 21ms/step - loss: 2.3790 - accuracy: 0.7870 - val_loss: 2.5932 - val_accuracy: 0.5757\n",
      "Epoch 23/100\n",
      "6669/6669 [==============================] - 139s 21ms/step - loss: 2.3775 - accuracy: 0.7886 - val_loss: 2.5943 - val_accuracy: 0.5716\n",
      "Epoch 24/100\n",
      "2862/6669 [===========>..................] - ETA: 1:13 - loss: 2.3771 - accuracy: 0.7889\n",
      "Epoch 00024: saving model to ./checkpoints/mobilenet_imbalanced_std_aug/cp-0024.ckpt\n",
      "6669/6669 [==============================] - 137s 21ms/step - loss: 2.3761 - accuracy: 0.7899 - val_loss: 2.5881 - val_accuracy: 0.5824\n",
      "Epoch 25/100\n",
      "6669/6669 [==============================] - 138s 21ms/step - loss: 2.3761 - accuracy: 0.7900 - val_loss: 2.5872 - val_accuracy: 0.5784\n",
      "Epoch 26/100\n",
      "6669/6669 [==============================] - 137s 21ms/step - loss: 2.3766 - accuracy: 0.7895 - val_loss: 2.5945 - val_accuracy: 0.5770\n",
      "Epoch 27/100\n",
      "6669/6669 [==============================] - 138s 21ms/step - loss: 2.3757 - accuracy: 0.7904 - val_loss: 2.5911 - val_accuracy: 0.5743\n",
      "Epoch 28/100\n",
      "6669/6669 [==============================] - 140s 21ms/step - loss: 2.3754 - accuracy: 0.7906 - val_loss: 2.5989 - val_accuracy: 0.5689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f66f8472890>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and save model\n",
    "train_model(mobilenet_std_aug, train_dataset_std_aug, validation_dataset, 'mobilenet_imbalanced_std_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/mobilenet_imbalanced_std_aug/assets\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join('models', 'mobilenet_imbalanced_std_aug')):\n",
    "    os.makedirs(os.path.join('models', 'mobilenet_imbalanced_std_aug'))\n",
    "    \n",
    "mobilenet_std_aug.save(os.path.join('models', 'mobilenet_imbalanced_std_aug'))\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) MOBILENET + BAGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "# New model for bagan augmentation\n",
    "mobilenet_bagan_aug = get_mobilenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Total Images:  7709\n",
      "Num. Train Images:  6939\n",
      "Num. Validation Images:  770\n"
     ]
    }
   ],
   "source": [
    "bagan_train_metadata = load_metadata('./metadata_output/bagan_train_metadata.txt')\n",
    "combined_bagan_train_metadata = []\n",
    "combined_bagan_train_metadata.extend(train_metadata)\n",
    "combined_bagan_train_metadata.extend(bagan_train_metadata)\n",
    "\n",
    "y_train_bagan = load_labels_with_encoder(combined_bagan_train_metadata, encoder)\n",
    "(bagan_train_paths_and_labels, bagan_validation_paths_and_labels) = shuffle_and_split_data(combined_bagan_train_metadata, y_train_bagan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redfine train dataset to include BAGAN samples\n",
    "# Convert training set into a TF dataset via generator\n",
    "train_dataset_bagan_aug = tf.data.Dataset.from_generator(\n",
    "    lambda: bagan_train_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train_bagan[0])]))\n",
    ")\n",
    "train_dataset_bagan_aug = train_dataset_bagan_aug.map(lambda x,y: load_image_data(x, y), \n",
    "                                                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset_bagan_aug = train_dataset_bagan_aug.cache()\n",
    "train_dataset_bagan_aug = train_dataset_bagan_aug.repeat()\n",
    "train_dataset_bagan_aug = train_dataset_bagan_aug.batch(BATCH_SIZE)\n",
    "train_dataset_bagan_aug = train_dataset_bagan_aug.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Convert validation set into a TF dataset via generator\n",
    "bagan_validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: bagan_validation_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train_bagan[0])]))\n",
    ")\n",
    "bagan_validation_dataset = bagan_validation_dataset.map(lambda x,y: load_image_data(x, y), \n",
    "                                                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "bagan_validation_dataset = bagan_validation_dataset.cache()\n",
    "bagan_validation_dataset = bagan_validation_dataset.repeat()\n",
    "bagan_validation_dataset = bagan_validation_dataset.batch(1)\n",
    "bagan_validation_dataset = bagan_validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Set test and train to dataset size...\n",
    "TRAIN_STEPS_PER_EPOCH = len(bagan_train_paths_and_labels)\n",
    "TEST_STEPS = len(bagan_validation_paths_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 6939 steps, validate for 770 steps\n",
      "Epoch 1/100\n",
      "6939/6939 [==============================] - 147s 21ms/step - loss: 2.4972 - accuracy: 0.6753 - val_loss: 2.6044 - val_accuracy: 0.5766\n",
      "Epoch 2/100\n",
      "6939/6939 [==============================] - 145s 21ms/step - loss: 2.3459 - accuracy: 0.8239 - val_loss: 2.5934 - val_accuracy: 0.5805\n",
      "Epoch 3/100\n",
      "6939/6939 [==============================] - 146s 21ms/step - loss: 2.3187 - accuracy: 0.8493 - val_loss: 2.5915 - val_accuracy: 0.5766\n",
      "Epoch 4/100\n",
      "6939/6939 [==============================] - 145s 21ms/step - loss: 2.3091 - accuracy: 0.8584 - val_loss: 2.5815 - val_accuracy: 0.5883\n",
      "Epoch 5/100\n",
      "6939/6939 [==============================] - 146s 21ms/step - loss: 2.3030 - accuracy: 0.8641 - val_loss: 2.5935 - val_accuracy: 0.5805\n",
      "Epoch 6/100\n",
      "4365/6939 [=================>............] - ETA: 51s - loss: 2.2990 - accuracy: 0.8679\n",
      "Epoch 00006: saving model to ./checkpoints/mobilenet_imbalanced_bagan_aug/cp-0006.ckpt\n",
      "6939/6939 [==============================] - 147s 21ms/step - loss: 2.2985 - accuracy: 0.8683 - val_loss: 2.6119 - val_accuracy: 0.5597\n",
      "Epoch 7/100\n",
      "6939/6939 [==============================] - 146s 21ms/step - loss: 2.2960 - accuracy: 0.8706 - val_loss: 2.5913 - val_accuracy: 0.5766\n",
      "Epoch 8/100\n",
      "6939/6939 [==============================] - 145s 21ms/step - loss: 2.2939 - accuracy: 0.8727 - val_loss: 2.5869 - val_accuracy: 0.5831\n",
      "Epoch 9/100\n",
      "6939/6939 [==============================] - 146s 21ms/step - loss: 2.2923 - accuracy: 0.8742 - val_loss: 2.5917 - val_accuracy: 0.5740\n",
      "Epoch 10/100\n",
      "6939/6939 [==============================] - 144s 21ms/step - loss: 2.2913 - accuracy: 0.8751 - val_loss: 2.5922 - val_accuracy: 0.5701\n",
      "Epoch 11/100\n",
      "6939/6939 [==============================] - 144s 21ms/step - loss: 2.2904 - accuracy: 0.8760 - val_loss: 2.5903 - val_accuracy: 0.5766\n",
      "Epoch 12/100\n",
      "1795/6939 [======>.......................] - ETA: 1:42 - loss: 2.2893 - accuracy: 0.8771\n",
      "Epoch 00012: saving model to ./checkpoints/mobilenet_imbalanced_bagan_aug/cp-0012.ckpt\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2893 - accuracy: 0.8771 - val_loss: 2.6052 - val_accuracy: 0.5610\n",
      "Epoch 13/100\n",
      "6939/6939 [==============================] - 146s 21ms/step - loss: 2.2731 - accuracy: 0.8933 - val_loss: 2.5819 - val_accuracy: 0.5870\n",
      "Epoch 14/100\n",
      "6939/6939 [==============================] - 146s 21ms/step - loss: 2.2589 - accuracy: 0.9076 - val_loss: 2.5796 - val_accuracy: 0.5883\n",
      "Epoch 15/100\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2550 - accuracy: 0.9114 - val_loss: 2.5685 - val_accuracy: 0.5961\n",
      "Epoch 16/100\n",
      "6939/6939 [==============================] - 149s 21ms/step - loss: 2.2521 - accuracy: 0.9143 - val_loss: 2.5893 - val_accuracy: 0.5753\n",
      "Epoch 17/100\n",
      "6162/6939 [=========================>....] - ETA: 15s - loss: 2.2508 - accuracy: 0.9156\n",
      "Epoch 00017: saving model to ./checkpoints/mobilenet_imbalanced_bagan_aug/cp-0017.ckpt\n",
      "6939/6939 [==============================] - 149s 22ms/step - loss: 2.2506 - accuracy: 0.9157 - val_loss: 2.5788 - val_accuracy: 0.5922\n",
      "Epoch 18/100\n",
      "6939/6939 [==============================] - 150s 22ms/step - loss: 2.2498 - accuracy: 0.9166 - val_loss: 2.5874 - val_accuracy: 0.5766\n",
      "Epoch 19/100\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2490 - accuracy: 0.9173 - val_loss: 2.5703 - val_accuracy: 0.5948\n",
      "Epoch 20/100\n",
      "6939/6939 [==============================] - 146s 21ms/step - loss: 2.2486 - accuracy: 0.9178 - val_loss: 2.5748 - val_accuracy: 0.5935\n",
      "Epoch 21/100\n",
      "6939/6939 [==============================] - 146s 21ms/step - loss: 2.2473 - accuracy: 0.9191 - val_loss: 2.5644 - val_accuracy: 0.6013\n",
      "Epoch 22/100\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2465 - accuracy: 0.9197 - val_loss: 2.5812 - val_accuracy: 0.5844\n",
      "Epoch 23/100\n",
      "3592/6939 [==============>...............] - ETA: 1:07 - loss: 2.2464 - accuracy: 0.9198\n",
      "Epoch 00023: saving model to ./checkpoints/mobilenet_imbalanced_bagan_aug/cp-0023.ckpt\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2465 - accuracy: 0.9198 - val_loss: 2.5653 - val_accuracy: 0.5987\n",
      "Epoch 24/100\n",
      "6939/6939 [==============================] - 150s 22ms/step - loss: 2.2460 - accuracy: 0.9202 - val_loss: 2.5716 - val_accuracy: 0.5909\n",
      "Epoch 25/100\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2465 - accuracy: 0.9197 - val_loss: 2.5784 - val_accuracy: 0.5883\n",
      "Epoch 26/100\n",
      "6939/6939 [==============================] - 149s 22ms/step - loss: 2.2454 - accuracy: 0.9208 - val_loss: 2.5531 - val_accuracy: 0.6104\n",
      "Epoch 27/100\n",
      "6939/6939 [==============================] - 149s 22ms/step - loss: 2.2450 - accuracy: 0.9212 - val_loss: 2.5792 - val_accuracy: 0.5857\n",
      "Epoch 28/100\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2450 - accuracy: 0.9212 - val_loss: 2.5713 - val_accuracy: 0.5961\n",
      "Epoch 29/100\n",
      "1021/6939 [===>..........................] - ETA: 1:58 - loss: 2.2448 - accuracy: 0.9214\n",
      "Epoch 00029: saving model to ./checkpoints/mobilenet_imbalanced_bagan_aug/cp-0029.ckpt\n",
      "6939/6939 [==============================] - 149s 21ms/step - loss: 2.2446 - accuracy: 0.9216 - val_loss: 2.5687 - val_accuracy: 0.5935\n",
      "Epoch 30/100\n",
      "6939/6939 [==============================] - 149s 21ms/step - loss: 2.2444 - accuracy: 0.9218 - val_loss: 2.5896 - val_accuracy: 0.5753\n",
      "Epoch 31/100\n",
      "6939/6939 [==============================] - 149s 21ms/step - loss: 2.2438 - accuracy: 0.9225 - val_loss: 2.5744 - val_accuracy: 0.5922\n",
      "Epoch 32/100\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2438 - accuracy: 0.9224 - val_loss: 2.5867 - val_accuracy: 0.5779\n",
      "Epoch 33/100\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2438 - accuracy: 0.9224 - val_loss: 2.5749 - val_accuracy: 0.5909\n",
      "Epoch 34/100\n",
      "5389/6939 [======================>.......] - ETA: 31s - loss: 2.2437 - accuracy: 0.9225\n",
      "Epoch 00034: saving model to ./checkpoints/mobilenet_imbalanced_bagan_aug/cp-0034.ckpt\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2436 - accuracy: 0.9226 - val_loss: 2.5784 - val_accuracy: 0.5844\n",
      "Epoch 35/100\n",
      "6939/6939 [==============================] - 148s 21ms/step - loss: 2.2431 - accuracy: 0.9231 - val_loss: 2.5857 - val_accuracy: 0.5779\n",
      "Epoch 36/100\n",
      "6939/6939 [==============================] - 149s 21ms/step - loss: 2.2431 - accuracy: 0.9231 - val_loss: 2.5854 - val_accuracy: 0.5766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f66af8f6350>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and save model\n",
    "train_model(mobilenet_bagan_aug, train_dataset_bagan_aug, bagan_validation_dataset, 'mobilenet_imbalanced_bagan_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/mobilenet_imbalanced_bagan_aug/assets\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join('models', 'mobilenet_imbalanced_bagan_aug')):\n",
    "    os.makedirs(os.path.join('models', 'mobilenet_imbalanced_bagan_aug'))\n",
    "    \n",
    "mobilenet_bagan_aug.save(os.path.join('models', 'mobilenet_imbalanced_bagan_aug'))\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) MOBILENET + SINGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "# New model for singan augmentation\n",
    "mobilenet_singan_aug = get_mobilenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Total Images:  7909\n",
      "Num. Train Images:  7119\n",
      "Num. Validation Images:  790\n"
     ]
    }
   ],
   "source": [
    "singan_train_metadata = load_metadata('./metadata_output/singan_train_metadata.txt')\n",
    "combined_train_metadata = []\n",
    "combined_train_metadata.extend(train_metadata)\n",
    "combined_train_metadata.extend(singan_train_metadata)\n",
    "\n",
    "y_train_singan = load_labels_with_encoder(combined_train_metadata, encoder)\n",
    "(singan_train_paths_and_labels, singan_validation_paths_and_labels) = shuffle_and_split_data(combined_train_metadata, y_train_singan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redfine train dataset to include SinGAN samples\n",
    "# Convert training set into a TF dataset via generator\n",
    "train_dataset_singan_aug = tf.data.Dataset.from_generator(\n",
    "    lambda: singan_train_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train_singan[0])]))\n",
    ")\n",
    "train_dataset_singan_aug = train_dataset_singan_aug.map(lambda x,y: load_image_data(x, y), \n",
    "                                                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset_singan_aug = train_dataset_singan_aug.cache()\n",
    "train_dataset_singan_aug = train_dataset_singan_aug.repeat()\n",
    "train_dataset_singan_aug = train_dataset_singan_aug.batch(BATCH_SIZE)\n",
    "train_dataset_singan_aug = train_dataset_singan_aug.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Convert validation set into a TF dataset via generator\n",
    "singan_validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: singan_validation_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train_singan[0])]))\n",
    ")\n",
    "singan_validation_dataset = singan_validation_dataset.map(lambda x,y: load_image_data(x, y), \n",
    "                                                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "singan_validation_dataset = singan_validation_dataset.cache()\n",
    "singan_validation_dataset = singan_validation_dataset.repeat()\n",
    "singan_validation_dataset = singan_validation_dataset.batch(1)\n",
    "singan_validation_dataset = singan_validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Set test and train to dataset size...\n",
    "TRAIN_STEPS_PER_EPOCH = len(singan_train_paths_and_labels)\n",
    "TEST_STEPS = len(singan_validation_paths_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 7119 steps, validate for 790 steps\n",
      "Epoch 1/100\n",
      "7119/7119 [==============================] - 154s 22ms/step - loss: 2.4986 - accuracy: 0.6737 - val_loss: 2.5459 - val_accuracy: 0.6291\n",
      "Epoch 2/100\n",
      "7119/7119 [==============================] - 152s 21ms/step - loss: 2.3447 - accuracy: 0.8250 - val_loss: 2.5637 - val_accuracy: 0.6127\n",
      "Epoch 3/100\n",
      "7119/7119 [==============================] - 153s 21ms/step - loss: 2.3207 - accuracy: 0.8473 - val_loss: 2.5529 - val_accuracy: 0.6215\n",
      "Epoch 4/100\n",
      "7119/7119 [==============================] - 150s 21ms/step - loss: 2.3101 - accuracy: 0.8573 - val_loss: 2.5599 - val_accuracy: 0.6127\n",
      "Epoch 5/100\n",
      "7119/7119 [==============================] - 151s 21ms/step - loss: 2.3003 - accuracy: 0.8669 - val_loss: 2.5381 - val_accuracy: 0.6304\n",
      "Epoch 6/100\n",
      "3466/7119 [=============>................] - ETA: 1:14 - loss: 2.2696 - accuracy: 0.8976\n",
      "Epoch 00006: saving model to ./checkpoints/mobilenet_imbalanced_singan_aug/cp-0006.ckpt\n",
      "7119/7119 [==============================] - 152s 21ms/step - loss: 2.2566 - accuracy: 0.9106 - val_loss: 2.5215 - val_accuracy: 0.6418\n",
      "Epoch 7/100\n",
      "7119/7119 [==============================] - 152s 21ms/step - loss: 2.2290 - accuracy: 0.9382 - val_loss: 2.5268 - val_accuracy: 0.6392\n",
      "Epoch 8/100\n",
      "7119/7119 [==============================] - 150s 21ms/step - loss: 2.2211 - accuracy: 0.9459 - val_loss: 2.5277 - val_accuracy: 0.6430\n",
      "Epoch 9/100\n",
      "7119/7119 [==============================] - 147s 21ms/step - loss: 2.2176 - accuracy: 0.9494 - val_loss: 2.5272 - val_accuracy: 0.6380\n",
      "Epoch 10/100\n",
      "7119/7119 [==============================] - 155s 22ms/step - loss: 2.2143 - accuracy: 0.9524 - val_loss: 2.5448 - val_accuracy: 0.6203\n",
      "Epoch 11/100\n",
      "6933/7119 [============================>.] - ETA: 3s - loss: 2.2132 - accuracy: 0.9534\n",
      "Epoch 00011: saving model to ./checkpoints/mobilenet_imbalanced_singan_aug/cp-0011.ckpt\n",
      "7119/7119 [==============================] - 154s 22ms/step - loss: 2.2131 - accuracy: 0.9535 - val_loss: 2.5422 - val_accuracy: 0.6215\n",
      "Epoch 12/100\n",
      "7119/7119 [==============================] - 152s 21ms/step - loss: 2.2119 - accuracy: 0.9546 - val_loss: 2.5289 - val_accuracy: 0.6367\n",
      "Epoch 13/100\n",
      "7119/7119 [==============================] - 154s 22ms/step - loss: 2.2110 - accuracy: 0.9555 - val_loss: 2.5316 - val_accuracy: 0.6418\n",
      "Epoch 14/100\n",
      "7119/7119 [==============================] - 153s 21ms/step - loss: 2.2099 - accuracy: 0.9566 - val_loss: 2.5388 - val_accuracy: 0.6304\n",
      "Epoch 15/100\n",
      "7119/7119 [==============================] - 154s 22ms/step - loss: 2.2097 - accuracy: 0.9568 - val_loss: 2.5303 - val_accuracy: 0.6367\n",
      "Epoch 16/100\n",
      "7119/7119 [==============================] - 149s 21ms/step - loss: 2.2087 - accuracy: 0.9576 - val_loss: 2.5240 - val_accuracy: 0.6405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f66ae6c5b90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and save model\n",
    "train_model(mobilenet_singan_aug, train_dataset_singan_aug, singan_validation_dataset, 'mobilenet_imbalanced_singan_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/mobilenet_imbalanced_singan_aug/assets\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join('models', 'mobilenet_imbalanced_singan_aug')):\n",
    "    os.makedirs(os.path.join('models', 'mobilenet_imbalanced_singan_aug'))\n",
    "    \n",
    "mobilenet_singan_aug.save(os.path.join('models', 'mobilenet_imbalanced_singan_aug'))\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "In this section, we will evaluate the models against the holdout data to compare performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_singan_eval = tf.keras.models.load_model('models/mobilenet_imbalanced_singan_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_imbalanced_eval = tf.keras.models.load_model('models/mobilenet_imbalanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_bagan_eval = tf.keras.models.load_model('models/mobilenet_imbalanced_bagan_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_imbalanced_std_aug_eval = tf.keras.models.load_model('models/mobilenet_imbalanced_std_aug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_metadata = load_metadata('./metadata_output/filtered_test_metadata.txt')\n",
    "y_train_holdout = load_labels_with_encoder(holdout_metadata, encoder)\n",
    "holdout_paths_and_labels = join_paths_and_labels(holdout_metadata, y_train_holdout)\n",
    "holdout_size = len(holdout_paths_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout dataset\n",
    "holdout_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: holdout_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train_holdout[0])]))\n",
    ")\n",
    "holdout_dataset = holdout_dataset.map(lambda x,y: load_image_data(x, y), \n",
    "                                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "holdout_dataset = holdout_dataset.cache()\n",
    "holdout_dataset = holdout_dataset.repeat()\n",
    "holdout_dataset = holdout_dataset.batch(BATCH_SIZE)\n",
    "holdout_dataset = holdout_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_against_holdout(model, dataset=holdout_dataset, steps=holdout_size):\n",
    "    return model.evaluate(dataset, steps=steps, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3208/3208 [==============================] - 56s 17ms/step - loss: 2.5563 - accuracy: 0.6100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5562510740132702, 0.6100374]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "evaluate_against_holdout(mobilenet_imbalanced_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3208/3208 [==============================] - 55s 17ms/step - loss: 2.5636 - accuracy: 0.6007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5636444811214534, 0.6006858]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Augmentation\n",
    "evaluate_against_holdout(mobilenet_imbalanced_std_aug_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3208/3208 [==============================] - 56s 17ms/step - loss: 2.5651 - accuracy: 0.5985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.565122230391847, 0.59850377]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BAGAN (Full)\n",
    "evaluate_against_holdout(mobilenet_bagan_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3208/3208 [==============================] - 56s 18ms/step - loss: 2.5268 - accuracy: 0.6378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.526828907374432, 0.63778055]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SinGAN\n",
    "evaluate_against_holdout(mobilenet_singan_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filtered samples: 283\n"
     ]
    }
   ],
   "source": [
    "# Filter only to interested categories\n",
    "filtered_holdout_paths_and_labels = [x for x in holdout_paths_and_labels if np.any(np.take(x[1], interested_indices))]\n",
    "num_filtered_holdout = len(filtered_holdout_paths_and_labels)\n",
    "print(\"Total filtered samples: {}\".format(num_filtered_holdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Holdout dataset\n",
    "holdout_interested_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: filtered_holdout_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train_holdout[0])]))\n",
    ")\n",
    "holdout_interested_dataset = holdout_interested_dataset.map(lambda x,y: load_image_data(x, y), \n",
    "                                                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "holdout_interested_dataset = holdout_interested_dataset.cache()\n",
    "holdout_interested_dataset = holdout_interested_dataset.repeat()\n",
    "holdout_interested_dataset = holdout_interested_dataset.batch(BATCH_SIZE)\n",
    "holdout_interested_dataset = holdout_interested_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/283 [==============================] - 5s 17ms/step - loss: 2.8230 - accuracy: 0.3428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.822999338379176, 0.34275618]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "evaluate_against_holdout(mobilenet_imbalanced_eval, holdout_interested_dataset, num_filtered_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/283 [==============================] - 5s 16ms/step - loss: 2.7825 - accuracy: 0.3816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.782473542243769, 0.38162544]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Augmentation\n",
    "evaluate_against_holdout(mobilenet_imbalanced_std_aug_eval, holdout_interested_dataset, num_filtered_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/283 [==============================] - 5s 17ms/step - loss: 2.4017 - accuracy: 0.7668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4016663497412583, 0.7667844]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BAGAN (Full)\n",
    "evaluate_against_holdout(mobilenet_bagan_eval, holdout_interested_dataset, num_filtered_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/283 [==============================] - 5s 17ms/step - loss: 2.4090 - accuracy: 0.7562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.408982995542115, 0.75618374]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SinGAN\n",
    "evaluate_against_holdout(mobilenet_singan_eval, holdout_interested_dataset, num_filtered_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-TF-GPU",
   "language": "python",
   "name": "conda-tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
