{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We will take a first-pass at evaluating or technique to start understanding its efficacy. We will existing CNN architectures and evaluate its performance on our interested categories with and without using our interested categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "print('TensorFlow Version: ', tf.__version__)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for training & validation\n",
    "INPUT_SHAPE = (64, 64, 3)\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.10\n",
    "TRAIN_STEPS_PER_EPOCH = 1200\n",
    "TEST_STEPS = 500\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utilities and helper functions\n",
    "# NOTE: Copied from clustering NB\n",
    "def load_metadata(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return [x.strip().split('\\t') for x in f.readlines()]\n",
    "    \n",
    "@tf.function\n",
    "def decode_img(image):\n",
    "    img = tf.image.decode_jpeg(image, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return tf.image.resize(img, [64, 64])\n",
    "\n",
    "@tf.function\n",
    "def load_image_data(path, label):\n",
    "    img_data = tf.io.read_file(path)\n",
    "    img = decode_img(img_data)\n",
    "    return img, label\n",
    "    \n",
    "def load_labels(metadata):\n",
    "    labels = np.array([x[1] for x in metadata])\n",
    "    distinct_labels = np.array([[x] for x in set(labels)])\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoder.fit(distinct_labels)\n",
    "    y_train = encoder.transform([[x] for x in labels])\n",
    "    return (y_train, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions for three models: (i) custom, simple CNN, (ii) MobileNet + FCs, and (iii) VGG16 + FCs\n",
    "def get_simplecnn(input_shape=INPUT_SHAPE):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(512, (3, 3), (1, 1), input_shape=input_shape, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(512, (2, 2), (1, 1), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(256, (2, 2), (1, 1), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(256, (2, 2), (1, 1), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(200, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "def get_mobilenet(input_shape=INPUT_SHAPE):\n",
    "    application = tf.keras.applications.MobileNet(input_shape=input_shape, include_top=False)\n",
    "    for i in range(len(application.layers)):\n",
    "        application.layers[i].trainable = False\n",
    "        \n",
    "    return tf.keras.Sequential([\n",
    "        application,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(200, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def get_vgg16(input_shape=INPUT_SHAPE):\n",
    "    application = tf.keras.applications.VGG16(input_shape=input_shape, include_top=False)\n",
    "    for i in range(len(application.layers)):\n",
    "        application.layers[i].trainable = False\n",
    "        \n",
    "    return tf.keras.Sequential([\n",
    "        application,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1024, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(200, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplecnn = get_simplecnn()\n",
    "# simplecnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenet_1.00_224 (Model)   (None, 2, 2, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               205000    \n",
      "=================================================================\n",
      "Total params: 3,433,864\n",
      "Trainable params: 205,000\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobilenet = get_mobilenet()\n",
    "mobilenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 = get_vgg16()\n",
    "# vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into memory...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Remove hardcoding\n",
    "print('Loading data into memory...')\n",
    "train_metadata = load_metadata('./metadata_output/train_metadata.txt')\n",
    "(y_train, encoder) = load_labels(train_metadata)\n",
    "\n",
    "# Interested indices for test data filtering\n",
    "interested_categories = ['n01882714', 'n04562935']\n",
    "interested_one_hot = encoder.transform([[x] for x in interested_categories])\n",
    "interested_indices = np.array([x[1] for x in np.argwhere(interested_one_hot == 1)])\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding sanity checks;\n",
    "# assert(len(train_metadata) == len(y_train))\n",
    "# assert(len(set(y_train)) == 200)\n",
    "assert(np.count_nonzero(y_train == 1) == len(train_metadata))\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) BASELINE MODEL: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Total Images:  70000\n",
      "Num. Train Images:  63000\n",
      "Num. Validation Images:  7000\n"
     ]
    }
   ],
   "source": [
    "# Get all data\n",
    "paths_and_labels = [(train_metadata[x][0], y_train[x]) for x in range(len(y_train))]\n",
    "print('Num. Total Images: ', len(paths_and_labels))\n",
    "\n",
    "# Split data into train and validation sets\n",
    "np.random.shuffle(paths_and_labels)\n",
    "num_validation = int(len(paths_and_labels) * VALIDATION_SPLIT)\n",
    "train_paths_and_labels = paths_and_labels[num_validation:]\n",
    "validation_paths_and_labels = paths_and_labels[:num_validation]\n",
    "print('Num. Train Images: ', len(train_paths_and_labels))\n",
    "print('Num. Validation Images: ', len(validation_paths_and_labels))\n",
    "\n",
    "# Convert training set into a TF dataset via generator\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train[0])]))\n",
    ")\n",
    "train_dataset = train_dataset.map(lambda x,y: load_image_data(x, y), \n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Convert validation set into a TF dataset via generator\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: validation_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train[0])]))\n",
    ")\n",
    "validation_dataset = validation_dataset.map(lambda x,y: load_image_data(x, y), \n",
    "                                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "validation_dataset = validation_dataset.cache()\n",
    "validation_dataset = validation_dataset.repeat()\n",
    "validation_dataset = validation_dataset.batch(1)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, validation_dataset, name):    \n",
    "    # Compile model                                                                                                      \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=4e-4),                                                           \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),                                  \n",
    "                  metrics=['accuracy'])      \n",
    "    \n",
    "    # Stop early if we're not making good progress                                                                           \n",
    "    early_stop_monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss',                                                                                              \n",
    "                                                          restore_best_weights=True,                                                                                       \n",
    "                                                          patience=10)   \n",
    "\n",
    "    # Prepare for checkpoints            \n",
    "    checkpoint_path = './checkpoints/' + name + '/cp-{epoch:04d}.ckpt'                                   \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                                                    \n",
    "                                                     verbose=1,                                                                                                   \n",
    "                                                     save_weights_only=True,                                                                                     \n",
    "                                                     save_freq=2500000)\n",
    "\n",
    "    # Tensorboard                                                                                                        \n",
    "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")                                              \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    history = model.fit(x=train_dataset,\n",
    "                        epochs=NUM_EPOCHS,                                                                                                  \n",
    "                        steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "                        callbacks=[tensorboard_callback, cp_callback, early_stop_monitor],\n",
    "                        use_multiprocessing=True,\n",
    "                        validation_steps=num_validation,\n",
    "                        validation_data=validation_dataset,\n",
    "                        shuffle=True)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on interesting inputs\n",
    "def evaluate_model(model, test_sets):\n",
    "    for test_set in test_sets:\n",
    "        X = test_set[0]\n",
    "        y = test_set[1]\n",
    "        model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1200 steps, validate for 7000 steps\n",
      "Epoch 1/150\n",
      "1200/1200 [==============================] - 175s 146ms/step - loss: 5.1824 - accuracy: 0.1424 - val_loss: 5.1321 - val_accuracy: 0.2057\n",
      "Epoch 2/150\n",
      "1200/1200 [==============================] - 131s 109ms/step - loss: 5.0634 - accuracy: 0.2659 - val_loss: 5.0899 - val_accuracy: 0.2476\n",
      "Epoch 3/150\n",
      "1200/1200 [==============================] - 98s 82ms/step - loss: 5.0123 - accuracy: 0.3164 - val_loss: 5.0728 - val_accuracy: 0.2629\n",
      "Epoch 4/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.9781 - accuracy: 0.3501 - val_loss: 5.0597 - val_accuracy: 0.2751\n",
      "Epoch 5/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.9498 - accuracy: 0.3795 - val_loss: 5.0546 - val_accuracy: 0.2789\n",
      "Epoch 6/150\n",
      "1200/1200 [==============================] - 100s 83ms/step - loss: 4.9313 - accuracy: 0.3975 - val_loss: 5.0463 - val_accuracy: 0.2836\n",
      "Epoch 7/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.9153 - accuracy: 0.4130 - val_loss: 5.0446 - val_accuracy: 0.2849\n",
      "Epoch 8/150\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.9007 - accuracy: 0.4270 - val_loss: 5.0395 - val_accuracy: 0.2903\n",
      "Epoch 9/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8870 - accuracy: 0.4412 - val_loss: 5.0355 - val_accuracy: 0.2920\n",
      "Epoch 10/150\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.8735 - accuracy: 0.4547 - val_loss: 5.0347 - val_accuracy: 0.2924\n",
      "Epoch 11/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8686 - accuracy: 0.4587 - val_loss: 5.0357 - val_accuracy: 0.2883\n",
      "Epoch 12/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8611 - accuracy: 0.4655 - val_loss: 5.0312 - val_accuracy: 0.2960\n",
      "Epoch 13/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.8556 - accuracy: 0.4701 - val_loss: 5.0307 - val_accuracy: 0.2940\n",
      "Epoch 14/150\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.8453 - accuracy: 0.4811 - val_loss: 5.0318 - val_accuracy: 0.2916\n",
      "Epoch 15/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8393 - accuracy: 0.4863 - val_loss: 5.0302 - val_accuracy: 0.2944\n",
      "Epoch 16/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.8378 - accuracy: 0.4874 - val_loss: 5.0298 - val_accuracy: 0.2917\n",
      "Epoch 17/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8328 - accuracy: 0.4924 - val_loss: 5.0301 - val_accuracy: 0.2926\n",
      "Epoch 18/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.8292 - accuracy: 0.4958 - val_loss: 5.0287 - val_accuracy: 0.2951\n",
      "Epoch 19/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8204 - accuracy: 0.5047 - val_loss: 5.0295 - val_accuracy: 0.2939\n",
      "Epoch 20/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8202 - accuracy: 0.5042 - val_loss: 5.0300 - val_accuracy: 0.2899\n",
      "Epoch 21/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8171 - accuracy: 0.5071 - val_loss: 5.0265 - val_accuracy: 0.2944\n",
      "Epoch 22/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8142 - accuracy: 0.5101 - val_loss: 5.0252 - val_accuracy: 0.2960\n",
      "Epoch 23/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8087 - accuracy: 0.5148 - val_loss: 5.0236 - val_accuracy: 0.2977\n",
      "Epoch 24/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8055 - accuracy: 0.5185 - val_loss: 5.0232 - val_accuracy: 0.2969\n",
      "Epoch 25/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8061 - accuracy: 0.5166 - val_loss: 5.0227 - val_accuracy: 0.2959\n",
      "Epoch 26/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.8020 - accuracy: 0.5210 - val_loss: 5.0217 - val_accuracy: 0.3009\n",
      "Epoch 27/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.8014 - accuracy: 0.5221 - val_loss: 5.0223 - val_accuracy: 0.2984\n",
      "Epoch 28/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.7936 - accuracy: 0.5295 - val_loss: 5.0221 - val_accuracy: 0.2980\n",
      "Epoch 29/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7938 - accuracy: 0.5285 - val_loss: 5.0230 - val_accuracy: 0.2977\n",
      "Epoch 30/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.7934 - accuracy: 0.5296 - val_loss: 5.0231 - val_accuracy: 0.2971\n",
      "Epoch 31/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7927 - accuracy: 0.5293 - val_loss: 5.0222 - val_accuracy: 0.2977\n",
      "Epoch 32/150\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.7884 - accuracy: 0.5330 - val_loss: 5.0214 - val_accuracy: 0.2971\n",
      "Epoch 33/150\n",
      " 660/1200 [===============>..............] - ETA: 11s - loss: 4.7864 - accuracy: 0.5357\n",
      "Epoch 00033: saving model to ./checkpoints/mobilenet_imbalanced/cp-0033.ckpt\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.7842 - accuracy: 0.5379 - val_loss: 5.0225 - val_accuracy: 0.2963\n",
      "Epoch 34/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7862 - accuracy: 0.5345 - val_loss: 5.0215 - val_accuracy: 0.2969\n",
      "Epoch 35/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.7841 - accuracy: 0.5380 - val_loss: 5.0213 - val_accuracy: 0.2957\n",
      "Epoch 36/150\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.7827 - accuracy: 0.5378 - val_loss: 5.0217 - val_accuracy: 0.2979\n",
      "Epoch 37/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.7773 - accuracy: 0.5436 - val_loss: 5.0182 - val_accuracy: 0.3009\n",
      "Epoch 38/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7788 - accuracy: 0.5414 - val_loss: 5.0220 - val_accuracy: 0.2956\n",
      "Epoch 39/150\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.7787 - accuracy: 0.5419 - val_loss: 5.0194 - val_accuracy: 0.2970\n",
      "Epoch 40/150\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.7775 - accuracy: 0.5429 - val_loss: 5.0204 - val_accuracy: 0.2994\n",
      "Epoch 41/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7739 - accuracy: 0.5464 - val_loss: 5.0210 - val_accuracy: 0.2971\n",
      "Epoch 42/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.7710 - accuracy: 0.5492 - val_loss: 5.0184 - val_accuracy: 0.2994\n",
      "Epoch 43/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.7731 - accuracy: 0.5467 - val_loss: 5.0205 - val_accuracy: 0.2976\n",
      "Epoch 44/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7713 - accuracy: 0.5490 - val_loss: 5.0199 - val_accuracy: 0.2946\n",
      "Epoch 45/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.7717 - accuracy: 0.5485 - val_loss: 5.0202 - val_accuracy: 0.2961\n",
      "Epoch 46/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7679 - accuracy: 0.5518 - val_loss: 5.0167 - val_accuracy: 0.3013\n",
      "Epoch 47/150\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.7685 - accuracy: 0.5508 - val_loss: 5.0190 - val_accuracy: 0.2997\n",
      "Epoch 48/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7673 - accuracy: 0.5525 - val_loss: 5.0174 - val_accuracy: 0.2999\n",
      "Epoch 49/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7664 - accuracy: 0.5528 - val_loss: 5.0210 - val_accuracy: 0.2939\n",
      "Epoch 50/150\n",
      "1200/1200 [==============================] - 103s 86ms/step - loss: 4.7663 - accuracy: 0.5535 - val_loss: 5.0194 - val_accuracy: 0.2953\n",
      "Epoch 51/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7627 - accuracy: 0.5569 - val_loss: 5.0188 - val_accuracy: 0.2979\n",
      "Epoch 52/150\n",
      "1200/1200 [==============================] - 103s 85ms/step - loss: 4.7631 - accuracy: 0.5562 - val_loss: 5.0180 - val_accuracy: 0.2983\n",
      "Epoch 53/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7632 - accuracy: 0.5553 - val_loss: 5.0189 - val_accuracy: 0.2961\n",
      "Epoch 54/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7626 - accuracy: 0.5560 - val_loss: 5.0177 - val_accuracy: 0.3003\n",
      "Epoch 55/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7593 - accuracy: 0.5594 - val_loss: 5.0174 - val_accuracy: 0.2984\n",
      "Epoch 56/150\n",
      "1200/1200 [==============================] - 102s 85ms/step - loss: 4.7572 - accuracy: 0.5610 - val_loss: 5.0170 - val_accuracy: 0.2999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcba8d5f5d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and save model\n",
    "train_model(mobilenet, train_dataset, validation_dataset, 'mobilenet_imbalanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dennis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/mobilenet_imbalanced/assets\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join('models', 'mobilenet_imbalanced')):\n",
    "    os.makedirs(os.path.join('models', 'mobilenet_imbalanced'))\n",
    "    \n",
    "mobilenet.save(os.path.join('models', 'mobilenet_imbalanced'))\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load test data, filter for interest, evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) MOBILENET + STANDARD AUGMENTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "mobilenet_std_aug = get_mobilenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: We have to somehow incorporate the below with tf.Datasets\n",
    "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#     rotation_range=45,\n",
    "#     width_shift_range=0.4,\n",
    "#     height_shift_range=0.4,\n",
    "#     zoom_range=[0.4, 1.6],\n",
    "#     horizontal_flip=True,\n",
    "#     brightness_range=(0.6, 1.4),\n",
    "#     fill_mode='nearest',\n",
    "# )\n",
    "\n",
    "# NOTE: Apply a map function to perform transformations rather than using ImageDataGen\n",
    "@tf.function\n",
    "def std_augment_image(img_tensor, label):\n",
    "    transformed = img_tensor\n",
    "    # Apply any transformation 80% of the time.\n",
    "    # Random rotation\n",
    "#     if tf.random.uniform([]) <= 0.8:\n",
    "#         angle = tf.random.uniform([]) * 45.0\n",
    "#         transformed = tfa.image.rotate(transformed, angle)\n",
    "    # Random zoom\n",
    "    if tf.random.uniform([]) <= 0.8:\n",
    "        crop_size = tf.random.uniform([], minval=0.4, maxval=0.8) * 64.0\n",
    "        transformed = tf.image.resize(tf.image.random_crop(transformed, [crop_size, crop_size, 3]), [64, 64])\n",
    "    # Random brightness adjustment\n",
    "    if tf.random.uniform([]) <= 0.8:\n",
    "        transformed = tf.image.random_brightness(transformed, 0.6)\n",
    "    # Random horizontal flip\n",
    "    transformed = tf.image.random_flip_up_down(transformed)\n",
    "    return (transformed, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redfine train dataset\n",
    "# Convert training set into a TF dataset via generator\n",
    "train_dataset_std_aug = tf.data.Dataset.from_generator(\n",
    "    lambda: train_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train[0])]))\n",
    ")\n",
    "train_dataset_std_aug = train_dataset_std_aug.map(lambda x,y: load_image_data(x, y), \n",
    "                                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset_std_aug = train_dataset_std_aug.cache()\n",
    "train_dataset_std_aug = train_dataset_std_aug.map(std_augment_image,\n",
    "                                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset_std_aug = train_dataset_std_aug.repeat()\n",
    "train_dataset_std_aug = train_dataset_std_aug.batch(BATCH_SIZE)\n",
    "train_dataset_std_aug = train_dataset_std_aug.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1200 steps, validate for 7000 steps\n",
      "Epoch 1/150\n",
      "1200/1200 [==============================] - 106s 88ms/step - loss: 5.2481 - accuracy: 0.0672 - val_loss: 5.1987 - val_accuracy: 0.1259\n",
      "Epoch 2/150\n",
      "1200/1200 [==============================] - 104s 87ms/step - loss: 5.1961 - accuracy: 0.1219 - val_loss: 5.1608 - val_accuracy: 0.1639\n",
      "Epoch 3/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1770 - accuracy: 0.1398 - val_loss: 5.1464 - val_accuracy: 0.1771\n",
      "Epoch 4/150\n",
      "1200/1200 [==============================] - 104s 87ms/step - loss: 5.1650 - accuracy: 0.1508 - val_loss: 5.1370 - val_accuracy: 0.1851\n",
      "Epoch 5/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1562 - accuracy: 0.1591 - val_loss: 5.1293 - val_accuracy: 0.1924\n",
      "Epoch 6/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1509 - accuracy: 0.1645 - val_loss: 5.1260 - val_accuracy: 0.1953\n",
      "Epoch 7/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1464 - accuracy: 0.1669 - val_loss: 5.1226 - val_accuracy: 0.1966\n",
      "Epoch 8/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1437 - accuracy: 0.1698 - val_loss: 5.1184 - val_accuracy: 0.2016\n",
      "Epoch 9/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1410 - accuracy: 0.1715 - val_loss: 5.1169 - val_accuracy: 0.2037\n",
      "Epoch 10/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1385 - accuracy: 0.1745 - val_loss: 5.1150 - val_accuracy: 0.2036\n",
      "Epoch 11/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1365 - accuracy: 0.1762 - val_loss: 5.1128 - val_accuracy: 0.2056\n",
      "Epoch 12/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1345 - accuracy: 0.1775 - val_loss: 5.1093 - val_accuracy: 0.2090\n",
      "Epoch 13/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1323 - accuracy: 0.1801 - val_loss: 5.1080 - val_accuracy: 0.2104\n",
      "Epoch 14/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1317 - accuracy: 0.1802 - val_loss: 5.1080 - val_accuracy: 0.2099\n",
      "Epoch 15/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1297 - accuracy: 0.1820 - val_loss: 5.1102 - val_accuracy: 0.2056\n",
      "Epoch 16/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1296 - accuracy: 0.1823 - val_loss: 5.1070 - val_accuracy: 0.2103\n",
      "Epoch 17/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1291 - accuracy: 0.1822 - val_loss: 5.1069 - val_accuracy: 0.2097\n",
      "Epoch 18/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1274 - accuracy: 0.1840 - val_loss: 5.1052 - val_accuracy: 0.2097\n",
      "Epoch 19/150\n",
      "1200/1200 [==============================] - 104s 87ms/step - loss: 5.1251 - accuracy: 0.1863 - val_loss: 5.1064 - val_accuracy: 0.2096\n",
      "Epoch 20/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1240 - accuracy: 0.1875 - val_loss: 5.1051 - val_accuracy: 0.2093\n",
      "Epoch 21/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1258 - accuracy: 0.1853 - val_loss: 5.1030 - val_accuracy: 0.2113\n",
      "Epoch 22/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1264 - accuracy: 0.1843 - val_loss: 5.1023 - val_accuracy: 0.2129\n",
      "Epoch 23/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1231 - accuracy: 0.1877 - val_loss: 5.1018 - val_accuracy: 0.2136\n",
      "Epoch 24/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1235 - accuracy: 0.1871 - val_loss: 5.1030 - val_accuracy: 0.2104\n",
      "Epoch 25/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1220 - accuracy: 0.1889 - val_loss: 5.1036 - val_accuracy: 0.2110\n",
      "Epoch 26/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1215 - accuracy: 0.1894 - val_loss: 5.1005 - val_accuracy: 0.2139\n",
      "Epoch 27/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1227 - accuracy: 0.1879 - val_loss: 5.1032 - val_accuracy: 0.2100\n",
      "Epoch 28/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1213 - accuracy: 0.1891 - val_loss: 5.1025 - val_accuracy: 0.2123\n",
      "Epoch 29/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1206 - accuracy: 0.1890 - val_loss: 5.1023 - val_accuracy: 0.2129\n",
      "Epoch 30/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1220 - accuracy: 0.1883 - val_loss: 5.1032 - val_accuracy: 0.2097\n",
      "Epoch 31/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1226 - accuracy: 0.1872 - val_loss: 5.1014 - val_accuracy: 0.2127\n",
      "Epoch 32/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1207 - accuracy: 0.1892 - val_loss: 5.1006 - val_accuracy: 0.2133\n",
      "Epoch 33/150\n",
      " 661/1200 [===============>..............] - ETA: 10s - loss: 5.1207 - accuracy: 0.1895\n",
      "Epoch 00033: saving model to ./checkpoints/mobilenet_imbalanced_std_aug/cp-0033.ckpt\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1194 - accuracy: 0.1904 - val_loss: 5.1006 - val_accuracy: 0.2117\n",
      "Epoch 34/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1192 - accuracy: 0.1906 - val_loss: 5.1021 - val_accuracy: 0.2106\n",
      "Epoch 35/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1182 - accuracy: 0.1912 - val_loss: 5.0996 - val_accuracy: 0.2130\n",
      "Epoch 36/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1199 - accuracy: 0.1898 - val_loss: 5.0979 - val_accuracy: 0.2159\n",
      "Epoch 37/150\n",
      "1200/1200 [==============================] - 104s 87ms/step - loss: 5.1174 - accuracy: 0.1926 - val_loss: 5.0971 - val_accuracy: 0.2169\n",
      "Epoch 38/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1178 - accuracy: 0.1918 - val_loss: 5.0968 - val_accuracy: 0.2170\n",
      "Epoch 39/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1200 - accuracy: 0.1897 - val_loss: 5.0963 - val_accuracy: 0.2149\n",
      "Epoch 40/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1170 - accuracy: 0.1924 - val_loss: 5.0971 - val_accuracy: 0.2143\n",
      "Epoch 41/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1180 - accuracy: 0.1913 - val_loss: 5.0962 - val_accuracy: 0.2163\n",
      "Epoch 42/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1163 - accuracy: 0.1926 - val_loss: 5.0941 - val_accuracy: 0.2179\n",
      "Epoch 43/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1180 - accuracy: 0.1909 - val_loss: 5.0978 - val_accuracy: 0.2133\n",
      "Epoch 44/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1156 - accuracy: 0.1939 - val_loss: 5.0963 - val_accuracy: 0.2173\n",
      "Epoch 45/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1155 - accuracy: 0.1939 - val_loss: 5.0960 - val_accuracy: 0.2170\n",
      "Epoch 46/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1159 - accuracy: 0.1935 - val_loss: 5.0965 - val_accuracy: 0.2146\n",
      "Epoch 47/150\n",
      "1200/1200 [==============================] - 104s 87ms/step - loss: 5.1166 - accuracy: 0.1924 - val_loss: 5.0969 - val_accuracy: 0.2144\n",
      "Epoch 48/150\n",
      "1200/1200 [==============================] - 104s 87ms/step - loss: 5.1162 - accuracy: 0.1928 - val_loss: 5.0977 - val_accuracy: 0.2146\n",
      "Epoch 49/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1162 - accuracy: 0.1929 - val_loss: 5.0945 - val_accuracy: 0.2191\n",
      "Epoch 50/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1144 - accuracy: 0.1948 - val_loss: 5.0934 - val_accuracy: 0.2181\n",
      "Epoch 51/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1135 - accuracy: 0.1958 - val_loss: 5.0953 - val_accuracy: 0.2159\n",
      "Epoch 52/150\n",
      "1200/1200 [==============================] - 104s 87ms/step - loss: 5.1171 - accuracy: 0.1918 - val_loss: 5.0978 - val_accuracy: 0.2129\n",
      "Epoch 53/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1151 - accuracy: 0.1938 - val_loss: 5.0964 - val_accuracy: 0.2137\n",
      "Epoch 54/150\n",
      "1200/1200 [==============================] - 104s 87ms/step - loss: 5.1165 - accuracy: 0.1926 - val_loss: 5.0943 - val_accuracy: 0.2169\n",
      "Epoch 55/150\n",
      "1200/1200 [==============================] - 104s 87ms/step - loss: 5.1129 - accuracy: 0.1965 - val_loss: 5.0943 - val_accuracy: 0.2173\n",
      "Epoch 56/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1136 - accuracy: 0.1954 - val_loss: 5.0940 - val_accuracy: 0.2171\n",
      "Epoch 57/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1166 - accuracy: 0.1924 - val_loss: 5.0946 - val_accuracy: 0.2163\n",
      "Epoch 58/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1149 - accuracy: 0.1939 - val_loss: 5.0958 - val_accuracy: 0.2150\n",
      "Epoch 59/150\n",
      "1200/1200 [==============================] - 105s 87ms/step - loss: 5.1147 - accuracy: 0.1944 - val_loss: 5.0934 - val_accuracy: 0.2164\n",
      "Epoch 60/150\n",
      "1200/1200 [==============================] - 105s 88ms/step - loss: 5.1119 - accuracy: 0.1971 - val_loss: 5.0958 - val_accuracy: 0.2156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcb24b6e390>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and save model\n",
    "train_model(mobilenet_std_aug, train_dataset_std_aug, validation_dataset, 'mobilenet_imbalanced_std_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/mobilenet_imbalanced_std_aug/assets\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join('models', 'mobilenet_imbalanced_std_aug')):\n",
    "    os.makedirs(os.path.join('models', 'mobilenet_imbalanced_std_aug'))\n",
    "    \n",
    "mobilenet_std_aug.save(os.path.join('models', 'mobilenet_imbalanced_std_aug'))\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) MOBILENET + SINGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "# New model for singan augmentation\n",
    "mobilenet_singan_aug = get_mobilenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redfine train dataset to include SinGAN samples\n",
    "# Convert training set into a TF dataset via generator\n",
    "train_dataset_std_aug = tf.data.Dataset.from_generator(\n",
    "    lambda: train_paths_and_labels,\n",
    "    (tf.string, tf.int32),\n",
    "    (tf.TensorShape([]), tf.TensorShape([len(y_train[0])]))\n",
    ")\n",
    "train_dataset_std_aug = train_dataset_std_aug.map(lambda x,y: load_image_data(x, y), \n",
    "                                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset_std_aug = train_dataset_std_aug.cache()\n",
    "train_dataset_std_aug = train_dataset_std_aug.repeat()\n",
    "train_dataset_std_aug = train_dataset_std_aug.batch(BATCH_SIZE)\n",
    "train_dataset_std_aug = train_dataset_std_aug.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-TF-GPU",
   "language": "python",
   "name": "conda-tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
